Entity,Code,Year,MMLU avg,Training computation (petaFLOP),Training dataset size,Organization
BLOOM,,2022,39.13,412000000,390000000000.0,"HuggingFace, BigScience"
BloombergGPT,,2023,39.18,212000000,708000000000.0,Bloomberg
Chinchilla,,2022,67.5,588000000,1400000000000.0,Google DeepMind
GLM-130B,,2022,44.8,312000000,400000000000.0,Tsinghua KEG
GPT-2 (finetuned),,2019,32.4,36000,4000000000.0,OpenAI
GPT-3 (davinci),,2020,43.9,393000000,374000000000.0,OpenAI
GPT-3.5,,2022,70.0,2580000000,4000000000000.0,OpenAI
GPT-4,,2023,86.4,21000000000,13000000000000.0,OpenAI
GPT-NeoX-20B,,2022,33.6,21200000,177000000000.0,Eleuther
Gemini Ultra,,2023,83.96,80000000000,13000000000000.0,Google DeepMind
Gopher (0.4B),,2021,25.7,751000,300000000000.0,Google DeepMind
Gopher (1.4B),,2021,27.3,2520000,300000000000.0,Google DeepMind
Gopher (280B),,2021,60.0,504000000,300000000000.0,Google DeepMind
Gopher (7B),,2021,29.5,12800000,300000000000.0,Google DeepMind
LLaMA (13B),,2023,46.9,78000000,1000000000000.0,Meta AI
LLaMA (33B),,2023,57.8,273000000,1400000000000.0,Meta AI
LLaMA (65B),,2023,63.4,548000000,1400000000000.0,Meta AI
LLaMA (7B),,2023,35.1,40200000,1000000000000.0,Meta AI
OPT,,2022,35.99,172000000,434000000000.0,Meta AI
PaLM (540B),,2022,69.3,2530000000,780000000000.0,Google Research
PaLM (62B),,2022,53.7,296000000,795000000000.0,Google Research
PaLM (62B+),,2022,62.8,493000000,1330000000000.0,Google Research
PaLM (8B),,2022,25.3,37400000,780000000000.0,Google Research
PaLM-2,,2023,78.3,8160000000,4000000000000.0,Google Research
code-davinci-002,,2022,68.3,2580000000,4000000000000.0,OpenAI
