Entity,Code,Year,MMLU avg,Training computation (petaFLOP),Training dataset size,Organization
BLOOM,,2022,39.13,412000000,3.9E+11,"HuggingFace, BigScience"
BloombergGPT,,2023,39.18,212000000,7.08E+11,Bloomberg
Chinchilla,,2022,67.5,588000000,1.4E+12,Google DeepMind
GLM-130B,,2022,44.8,312000000,4E+11,Tsinghua KEG
GPT-2 (finetuned),,2019,32.4,36000,4000000000,OpenAI
GPT-3 (davinci),,2020,43.9,393000000,3.74E+11,OpenAI
GPT-3.5,,2022,70,2580000000,4E+12,OpenAI
GPT-4,,2023,86.4,21000000000,1.3E+13,OpenAI
GPT-NeoX-20B,,2022,33.6,21200000,1.77E+11,Eleuther
Gemini Ultra,,2023,83.96,80000000000,1.3E+13,Google DeepMind
Gopher (0.4B),,2021,25.7,751000,3E+11,Google DeepMind
Gopher (1.4B),,2021,27.3,2520000,3E+11,Google DeepMind
Gopher (280B),,2021,60,504000000,3E+11,Google DeepMind
Gopher (7B),,2021,29.5,12800000,3E+11,Google DeepMind
LLaMA (13B),,2023,46.9,78000000,1E+12,Meta AI
LLaMA (33B),,2023,57.8,273000000,1.4E+12,Meta AI
LLaMA (65B),,2023,63.4,548000000,1.4E+12,Meta AI
LLaMA (7B),,2023,35.1,40200000,1E+12,Meta AI
OPT,,2022,35.99,172000000,4.34E+11,Meta AI
PaLM (540B),,2022,69.3,2530000000,7.8E+11,Google Research
PaLM (62B),,2022,53.7,296000000,7.95E+11,Google Research
PaLM (62B+),,2022,62.8,493000000,1.33E+12,Google Research
PaLM (8B),,2022,25.3,37400000,7.8E+11,Google Research
PaLM-2,,2023,78.3,8160000000,4E+12,Google Research
code-davinci-002,,2022,68.3,2580000000,4E+12,OpenAI
